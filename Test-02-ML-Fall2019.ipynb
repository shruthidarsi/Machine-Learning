{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 02: Principle Component Analysis\n",
    "\n",
    "**Due: Friday 12/13 (by midnight)**\n",
    "\n",
    "Name: Shruthi Madishetty\n",
    "\n",
    "CWID: 50239178\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T14:17:26.371350Z",
     "start_time": "2019-12-02T14:17:25.858613Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import sklearn\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T14:17:26.375966Z",
     "start_time": "2019-12-02T14:17:26.372709Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (10, 8) # set default figure size, 10in by 8in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 \n",
    "\n",
    "Load the MNIST data, and split it into a training\n",
    "set, a validation set, and a test set (e.g., use the first 50,000 instances for\n",
    "training, the next 10,000 for validation, and the last 10,000 for testing).\n",
    "You can load the MNIST data from Scikit-Learn as shown in the cell given to you below.\n",
    "\n",
    "Then train various classifiers, such as a Random Forest classifier, an Extra-\n",
    "Trees classifier, and an SVM. Next, try to combine them into an ensemble\n",
    "that outperforms them all on the validation set, using a soft or hard voting\n",
    "classifier. Once you have found one, try it on the test set. How much better\n",
    "does it perform compared to the individual classifiers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Load mnist data\n",
    "    As part of this step,\n",
    "    1. Loaded data into mnist variable\n",
    "    2. Divided features into x and y as input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T14:17:41.973479Z",
     "start_time": "2019-12-02T14:17:26.377346Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets  import fetch_openml\n",
    "\n",
    "mnist = fetch_openml('mnist_784')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividing x and y as input and output features for convenience, 'y' as mnist target and 'x' as remaining features\n",
    "x = mnist.data\n",
    "y = mnist.target.astype(np.int64)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Split data\n",
    "\n",
    "    As part of this step,There are total 70000 * 784 data in mnist\n",
    "    1. Divided first 10000 rows into validation set ( as X_validation and y_validation)\n",
    "    2. Then divided 10000 to Test set ( as X_test and y_test)\n",
    "    3. Remaining 50000 goes to training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T14:17:41.984234Z",
     "start_time": "2019-12-02T14:17:41.974936Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train1, X_validation, y_train1, y_validation = train_test_split(x, y, test_size=10000, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train1, y_train1, test_size=10000, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - PCA (as fitering) to reduce features\n",
    "    I am trying to find No. of components required for model since 784 features with 50000 samples are large dataset. And we need to reduce it to train the model with data.\n",
    "    1. Finding No. of components with 50% variance taken using PCA.\n",
    "    2. Transform Training set to the PCA components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(0.5).fit(X_train)\n",
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_components = pca.transform(X_train)\n",
    "validate_components = pca.transform(X_validation)\n",
    "test_components = pca.transform(X_test)\n",
    "\n",
    "print(train_components.shape, y_train.shape, X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Create models and fit \n",
    "    Creating 3 models(Random-forests, extra-tree-forest, and SVM) and using them as an estimator into voting classification(Ensamble method)\n",
    "    \n",
    "    Total 4 classifiers are created and I used HARD voting classification here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T14:17:42.207689Z",
     "start_time": "2019-12-02T14:17:41.985202Z"
    }
   },
   "outputs": [],
   "source": [
    "#fit the data into different models\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "import time\n",
    "\n",
    "# random-forests-classifier\n",
    "rf_clf = RandomForestClassifier(max_depth=10, random_state=42)\n",
    "\n",
    "#extra-tree-classifier\n",
    "et_classifier = ExtraTreesClassifier(n_estimators=10, random_state=42)\n",
    "\n",
    "#svm-classifiers\n",
    "svm_clf = SVC(gamma=100.0, C=1.0, random_state=42)\n",
    "\n",
    "#voting-classifiers\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('etc', et_classifier), ('rfc', rf_clf), ('svc', svm_clf)],\n",
    "    voting='hard'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Test the models with score/accuracy\n",
    "     All above created models are fit with training data, and try to findout the score value on validation and test dataset.\n",
    "     1. I Used dictionary to store all the classifiers and looping them through to get one-by-one value.\n",
    "     2. All the models produced score values 0.8517, 0.9046, 0.1152 and 0.9176 respectively.\n",
    "     3. We can clearly say that voting classifer have better results than all other classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_list = {\"random_forest_classifier\": rf_clf, \"extra-tree-classifier\":et_classifier, \n",
    "            \"svm_classifier\": svm_clf, \"voting_classifier\":voting_clf}\n",
    "\n",
    "for name, cassification in clf_list.items() :\n",
    "    print(\"--------\", name, \"-----------\")\n",
    "    cassification.fit(train_components, y_train)\n",
    "    y_pred = cassification.predict(test_components)\n",
    "    print(\"Validation Score\", cassification.score(validate_components, y_validation))\n",
    "    print(\"Prediction \", cassification.score(test_components, y_pred)*100 ,\"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional step\n",
    "    I added another way to find out the score value that can be applicable on whole features instead of filtered components. The accuracy that I got here are almost similar(slightly higher for all)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "print(cross_val_score(et_classifier, validate_components, y_validation, scoring='accuracy', cv=5))\n",
    "print(cross_val_score(rf_clf, validate_components, y_validation, scoring='accuracy', cv=5))\n",
    "print(cross_val_score(svm_clf, validate_components, y_validation, scoring='accuracy', cv=5))\n",
    "print(cross_val_score(voting_clf, validate_components, y_validation, scoring='accuracy', cv=5))\n",
    "\n",
    "print(cross_val_score(et_classifier, X_test, y_test, scoring='accuracy', cv=5))\n",
    "print(cross_val_score(rf_clf, X_test, y_test, scoring='accuracy', cv=5))\n",
    "print(cross_val_score(svm_clf, X_test, y_test, scoring='accuracy', cv=5))\n",
    "print(cross_val_score(voting_clf, X_test, y_test, scoring='accuracy', cv=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 Principle Component Analysis\n",
    "The following code generates 40 3-dimensional samples randomly drawn from a (multivariate) gaussian\n",
    "distribution.  This simply means that we have 3 variables, or dimensions.  Each dimension for the 40\n",
    "samples is drawn with some sample mean, and some standard deviation.  We generate 2 different classes\n",
    "where one half (i.e. 20) samples of our data are from class 1 and the other half are from class 2.\n",
    "We use the following sample means and covariance matrices to generate the data:\n",
    "\n",
    "$$\n",
    "\\mu_1 =\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "\\end{bmatrix}\n",
    "\\mu_2 =\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "\\end{bmatrix}\n",
    "\\textrm{(sample means)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Sigma_1 =\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "\\Sigma_2 =\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "\\textrm{(covariance matrices)}\n",
    "$$\n",
    "\n",
    "This basically means the mean value for each of the 3 dimensions for the class 1 is 0, and for class 2\n",
    "it is 1.  The covariance matrices are similar to the ones we discussed a bit when looking at PCA, for\n",
    "now you can simply think of them as specifying the amount of standard deviation we will have for each\n",
    "dimension.  Here is the code to generate two $20 \\times 3$ classes of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T14:17:42.942788Z",
     "start_time": "2019-12-02T14:17:42.209068Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1) # random seed for consistency\n",
    "\n",
    "mu_vec1 = np.array([0,0,0])\n",
    "cov_mat1 = np.array([[1,0,0],[0,1,0],[0,0,1]])\n",
    "class1_sample = np.random.multivariate_normal(mu_vec1, cov_mat1, 20)\n",
    "assert class1_sample.shape == (20,3), \"The matrix has not the dimensions 20x3\"\n",
    "\n",
    "mu_vec2 = np.array([1,1,1])\n",
    "cov_mat2 = np.array([[1,0,0],[0,1,0],[0,0,1]])\n",
    "class2_sample = np.random.multivariate_normal(mu_vec2, cov_mat2, 20)\n",
    "assert class1_sample.shape == (20, 3), \"The matrix has not the dimensions 20x3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to get a rough idea of how the samples of our two classes are distributed, let us plot them in\n",
    "a 3D scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T14:17:43.449261Z",
     "start_time": "2019-12-02T14:17:42.946870Z"
    }
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "ax.plot(class1_sample[:,0], class1_sample[:,1], class1_sample[:,2],\n",
    "        'o', markersize=8, color='blue', alpha=0.5, label='class1')\n",
    "ax.plot(class2_sample[:,0], class2_sample[:,1], class2_sample[:,2],\n",
    "        '^', markersize=8, alpha=0.5, color='red', label='class2')\n",
    "\n",
    "plt.title('Samples for class 1 and class 2')\n",
    "ax.legend(loc='upper right')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will concatenate both of our 20 class samples into a single numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T14:17:43.456641Z",
     "start_time": "2019-12-02T14:17:43.450931Z"
    }
   },
   "outputs": [],
   "source": [
    "all_samples = np.concatenate((class1_sample, class2_sample), axis=0)\n",
    "assert all_samples.shape == (40,3), \"The matrix has not the dimensions 3x40\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the methods we did in our lectures and assignments, perform a principal component analysis\n",
    "(PCA) on the combined set of 40 samples of data.  Reduce the dimensions too 2 dimensions and plot\n",
    "the resulting PCA dimensionality reduction on a 2D figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T14:17:43.595243Z",
     "start_time": "2019-12-02T14:17:43.458179Z"
    }
   },
   "outputs": [],
   "source": [
    "# step 1, normalize the features...\n",
    "\n",
    "mean1 = np.mean(all_samples)\n",
    "\n",
    "mu = np.mean(all_samples, axis=0)\n",
    "sd =  np.std(all_samples, axis=0)\n",
    "\n",
    "X_norm = all_samples - mu\n",
    "X_norm_scaled = (all_samples - mu) / sd\n",
    "\n",
    "# extract m, number of samples\n",
    "# extract n, number of features/dimensions\n",
    "m,n = X_norm_scaled.shape\n",
    "print(m,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T14:17:43.665248Z",
     "start_time": "2019-12-02T14:17:43.596860Z"
    }
   },
   "outputs": [],
   "source": [
    "# step 2, do the PCA\n",
    "def pca(X):\n",
    "    Sigma = np.empty( (n, n) )\n",
    "    U = S = V = np.zeros( (n, n) )\n",
    "    \n",
    "    Sigma = np.dot(X.T , X) / m\n",
    "    U , S, V = np.linalg.svd(Sigma)\n",
    "    \n",
    "    # return the U matrix of the principal components, and the S vector of the variance measures\n",
    "    return U, S\n",
    "\n",
    "U, S = pca(X_norm_scaled)\n",
    "print(U)\n",
    "print(S)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T14:17:43.756152Z",
     "start_time": "2019-12-02T14:17:43.667577Z"
    }
   },
   "outputs": [],
   "source": [
    "# step 3, reduce to 2 dimensions and project back onto the 2 dimensions\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2, whiten=True)\n",
    "pca.fit(X_norm_scaled)\n",
    "print(pca.components_)\n",
    "print(pca.explained_variance_)\n",
    "\n",
    "X_pca = pca.transform(X_norm_scaled)\n",
    "print(\"original shape:   \", X_norm_scaled.shape)\n",
    "print(\"transformed shape:\", X_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T14:17:43.848113Z",
     "start_time": "2019-12-02T14:17:43.757462Z"
    }
   },
   "outputs": [],
   "source": [
    "# finally, plot the recovered data\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "ax.plot(all_samples[:,0],all_samples[:,1],\n",
    "        'o', markersize=8, color='blue', alpha=0.5, label='PCA-Dim-Reduction')\n",
    "\n",
    "plt.title('3D-To-2D-Dimension-reduction')\n",
    "ax.legend(loc='lower left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
